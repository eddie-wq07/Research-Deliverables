{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task #2: Merging PatentsView, DISCERN, and Clinical Trials\n",
    "## Biopharma Firm's AI Capabilities via Patent Applications\n",
    "#### Edward Jung\n",
    "\n",
    "**Objective:** Construct a firm-year dataset of AI-related patent applications for firms conducting clinical trials.\n",
    "\n",
    "**Key Differences from Task #1:**\n",
    "- Focus on **patent applications** (not just granted patents)\n",
    "- Use **g_application** table (captures earliest innovation timing)\n",
    "- Map to **gvkey** using DISCERN 2 database\n",
    "- Time period: **2000-2025**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Architecture Design](#1-data-architecture-design)\n",
    "2. [Data Import & Setup](#2-data-import--setup)\n",
    "3. [PatentsView: Patent Applications (2000-2025)](#3-patentsview-patent-applications-2000-2025)\n",
    "4. [DISCERN 2: Firm-to-GVKEY Mapping](#4-discern-2-firm-to-gvkey-mapping)\n",
    "5. [AI Classification Logic](#5-ai-classification-logic)\n",
    "6. [Firm-Year Aggregation](#6-firm-year-aggregation)\n",
    "7. [Merge with Clinical Trials Dataset](#7-merge-with-clinical-trials-dataset)\n",
    "8. [Export Final Datasets](#8-export-final-datasets)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Architecture Design\n",
    "\n",
    "### Recommended Two-Layer Approach\n",
    "\n",
    "#### Layer 1: **Patent-Level Dataset** (Intermediate)\n",
    "One row per patent application\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| application_id | str | Unique patent application identifier |\n",
    "| patent_id | str | Patent ID if granted (may be null) |\n",
    "| filing_date | date | Application filing date |\n",
    "| filing_year | int | Year extracted from filing_date |\n",
    "| assignee_name | str | Raw assignee/applicant name |\n",
    "| gvkey | str | GVKEY from DISCERN 2 mapping |\n",
    "| is_ai | bool | Binary: 1 if AI-related, 0 otherwise |\n",
    "| ai_method | str | How AI was identified: 'cpc', 'keyword', or 'both' |\n",
    "| ai_cpc_codes | str | Comma-separated AI CPC codes found |\n",
    "| ai_keywords | str | AI keywords matched in title/abstract |\n",
    "| title | str | Patent application title |\n",
    "| abstract | str | Patent abstract text |\n",
    "\n",
    "#### Layer 2: **Firm-Year Dataset** (Final Output)\n",
    "One row per gvkey-year combination\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| gvkey | str | Firm identifier from Compustat |\n",
    "| year | int | Calendar year |\n",
    "| total_applications | int | Total patent applications filed |\n",
    "| ai_applications | int | AI-related applications |\n",
    "| ai_share | float | ai_applications / total_applications |\n",
    "| ai_dummy | int | 1 if ai_applications > 0, else 0 |\n",
    "\n",
    "### Memory Efficiency Strategy\n",
    "\n",
    "1. **Chunked Reading:** Process large TSV files in chunks (100k-500k rows)\n",
    "2. **Categorical Types:** Convert repetitive strings (year, gvkey) to category dtype\n",
    "3. **Early Filtering:** Filter to 2000-2025 and relevant firms before loading full data\n",
    "4. **DuckDB Pre-filtering:** Use SQL to filter before pandas import\n",
    "5. **Column Pruning:** Drop unnecessary columns immediately after import\n",
    "6. **Incremental Processing:** Process year-by-year if memory constrained\n",
    "\n",
    "### Why Two Layers?\n",
    "\n",
    "- **Patent-level:** Allows validation, spot-checking, manual review\n",
    "- **Firm-year:** Research-ready for regression analysis\n",
    "- **Reproducibility:** Can regenerate firm-year from patent-level if needed\n",
    "- **Flexibility:** Easy to add new metrics without re-processing raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For large file processing\n",
    "import duckdb\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Clinical Trials Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clinical trials sample\n",
    "clinical_trials = pd.read_csv('clinical_trial_sample (1).csv')\n",
    "\n",
    "print(f\"Clinical Trials Dataset Shape: {clinical_trials.shape}\")\n",
    "print(f\"\\nColumns: {clinical_trials.columns.tolist()}\")\n",
    "print(f\"\\nDate range: {clinical_trials['start_year'].min()} - {clinical_trials['start_year'].max()}\")\n",
    "print(f\"\\nUnique firms (gvkey): {clinical_trials['gvkey_sponsor'].nunique()}\")\n",
    "print(f\"Unique NCT IDs: {clinical_trials['nct_id'].nunique()}\")\n",
    "\n",
    "# Display sample\n",
    "clinical_trials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique firms for filtering patent data\n",
    "unique_gvkeys = clinical_trials['gvkey_sponsor'].dropna().unique()\n",
    "unique_sponsors = clinical_trials['sponsor_name'].dropna().unique()\n",
    "\n",
    "print(f\"Number of unique GVKEYs: {len(unique_gvkeys)}\")\n",
    "print(f\"Number of unique sponsor names: {len(unique_sponsors)}\")\n",
    "print(f\"\\nSample sponsor names:\")\n",
    "for sponsor in unique_sponsors[:10]:\n",
    "    print(f\"  - {sponsor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PatentsView: Patent Applications (2000-2025)\n",
    "\n",
    "### Key PatentsView Tables for Applications\n",
    "\n",
    "According to Task #2 requirements, we need **application-level** data:\n",
    "\n",
    "1. **g_application** - Core application information\n",
    "   - `application_id`: Unique identifier\n",
    "   - `filing_date`: Application date (KEY for temporal alignment)\n",
    "   - `patent_id`: Patent ID if granted (may be NULL for pending)\n",
    "   \n",
    "2. **pg_applicant_not_disambiguated** - Applicant names (for DISCERN matching)\n",
    "   - `application_id`: Links to g_application\n",
    "   - `applicant_organization`: Company name (raw, not disambiguated)\n",
    "   \n",
    "3. **g_cpc_current** - CPC classification codes (for AI identification)\n",
    "   - Can link via `patent_id` (only for granted applications)\n",
    "   \n",
    "4. **g_us_application_citation** or **g_patent_abstract** - For keyword search\n",
    "\n",
    "### Data Download Strategy\n",
    "\n",
    "**Option A: Download from PatentsView bulk data**\n",
    "- Base URL: https://s3.amazonaws.com/data.patentsview.org/download/\n",
    "- Files: `g_application.tsv.zip`, `pg_applicant_not_disambiguated.tsv.zip`\n",
    "\n",
    "**Option B: Use existing Task1 data + supplement**\n",
    "- Task1 has granted patents (2021)\n",
    "- Need to download application-specific tables\n",
    "\n",
    "**Recommended: Option A** (complete application data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for downloading PatentsView data\n",
    "def download_patentsview_table(table_name, data_dir='../Task1', overwrite=False):\n",
    "    \"\"\"\n",
    "    Download and extract PatentsView table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    table_name : str\n",
    "        Name of table (e.g., 'g_application')\n",
    "    data_dir : str\n",
    "        Directory to save files\n",
    "    overwrite : bool\n",
    "        Whether to re-download if file exists\n",
    "    \"\"\"\n",
    "    base_url = \"https://s3.amazonaws.com/data.patentsview.org/download\"\n",
    "    zip_url = f\"{base_url}/{table_name}.tsv.zip\"\n",
    "    filename = f\"{table_name}.tsv\"\n",
    "    filepath = Path(data_dir) / filename\n",
    "    \n",
    "    # Create directory if doesn't exist\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if filepath.exists() and not overwrite:\n",
    "        print(f\"✓ {filename} already exists\")\n",
    "        return str(filepath)\n",
    "    \n",
    "    print(f\"Downloading {table_name}...\")\n",
    "    zip_path = filepath.with_suffix('.tsv.zip')\n",
    "    \n",
    "    try:\n",
    "        # Download ZIP file\n",
    "        urlretrieve(zip_url, zip_path)\n",
    "        \n",
    "        # Extract TSV\n",
    "        with ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        \n",
    "        # Remove ZIP file\n",
    "        zip_path.unlink()\n",
    "        \n",
    "        print(f\"✓ Downloaded and extracted {filename}\")\n",
    "        return str(filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Download utility loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required tables (THIS MAY TAKE SEVERAL MINUTES)\n",
    "required_tables = [\n",
    "    'g_application',                    # Core application data\n",
    "    'pg_applicant_not_disambiguated',   # Applicant names for matching\n",
    "    'g_cpc_current',                    # Already downloaded in Task1\n",
    "    'g_patent_abstract'                 # Already downloaded in Task1\n",
    "]\n",
    "\n",
    "print(\"Checking/downloading required PatentsView tables...\\n\")\n",
    "for table in required_tables:\n",
    "    download_patentsview_table(table)\n",
    "    \n",
    "print(\"\\n✓ All required tables ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-Efficient Import Using DuckDB\n",
    "\n",
    "**Why DuckDB?**\n",
    "- Handles multi-GB files without loading into memory\n",
    "- SQL interface for filtering before pandas import\n",
    "- Fast aggregations and joins\n",
    "- No server setup required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect('task2_patents.ddb')\n",
    "\n",
    "print(\"DuckDB initialized: task2_patents.ddb\")\n",
    "print(f\"Database location: {os.path.abspath('task2_patents.ddb')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import g_application table\n",
    "print(\"Importing g_application (this may take 2-3 minutes)...\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE g_application AS \n",
    "    SELECT * FROM read_csv('../Task1/g_application.tsv', \n",
    "                           delim='\\t', \n",
    "                           header=true,\n",
    "                           all_varchar=true)\n",
    "\"\"\")\n",
    "\n",
    "# Check import\n",
    "result = con.execute(\"SELECT COUNT(*) as total FROM g_application\").fetchdf()\n",
    "print(f\"✓ Total applications loaded: {result['total'].iloc[0]:,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample records:\")\n",
    "con.execute(\"SELECT * FROM g_application LIMIT 3\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to 2000-2025 applications\n",
    "print(\"Filtering to 2000-2025 application years...\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW applications_2000_2025 AS\n",
    "    SELECT *,\n",
    "           CAST(SUBSTRING(filing_date, 1, 4) AS INTEGER) as filing_year\n",
    "    FROM g_application\n",
    "    WHERE filing_date >= '2000-01-01' \n",
    "      AND filing_date <= '2025-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# Count applications by year\n",
    "yearly_counts = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        filing_year,\n",
    "        COUNT(*) as application_count\n",
    "    FROM applications_2000_2025\n",
    "    GROUP BY filing_year\n",
    "    ORDER BY filing_year\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nApplications by year (2000-2025):\")\n",
    "print(yearly_counts)\n",
    "print(f\"\\nTotal applications (2000-2025): {yearly_counts['application_count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import applicant names\n",
    "print(\"Importing pg_applicant_not_disambiguated...\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE pg_applicant AS \n",
    "    SELECT * FROM read_csv('../Task1/pg_applicant_not_disambiguated.tsv', \n",
    "                           delim='\\t', \n",
    "                           header=true,\n",
    "                           all_varchar=true)\n",
    "\"\"\")\n",
    "\n",
    "result = con.execute(\"SELECT COUNT(*) as total FROM pg_applicant\").fetchdf()\n",
    "print(f\"✓ Total applicant records: {result['total'].iloc[0]:,}\")\n",
    "\n",
    "# Show sample with organization names\n",
    "print(\"\\nSample applicant records:\")\n",
    "con.execute(\"\"\"\n",
    "    SELECT * FROM pg_applicant \n",
    "    WHERE applicant_organization IS NOT NULL \n",
    "    LIMIT 5\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DISCERN 2: Firm-to-GVKEY Mapping\n",
    "\n",
    "### DISCERN 2 Overview\n",
    "\n",
    "**Reference:** https://zenodo.org/records/13619821\n",
    "\n",
    "DISCERN 2 provides:\n",
    "- Mapping between patent assignees and Compustat GVKEY\n",
    "- Handles name variations and disambiguation\n",
    "- Time-varying firm identifiers\n",
    "\n",
    "### Required DISCERN 2 Files\n",
    "\n",
    "1. **Main mapping file:** Links assignee names → gvkey\n",
    "2. **Time-varying mappings:** Tracks firm changes over time (mergers, acquisitions)\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "**Option A: Direct Name Matching**\n",
    "- Match `pg_applicant.applicant_organization` to DISCERN assignee names\n",
    "- Join on cleaned/standardized names\n",
    "\n",
    "**Option B: Use Existing Clinical Trials Mapping**\n",
    "- Clinical trials dataset already has sponsor_name → gvkey mapping\n",
    "- Use this as ground truth for fuzzy matching to patent applicants\n",
    "\n",
    "**Recommended: Hybrid Approach**\n",
    "1. Start with clinical trials sponsor names\n",
    "2. Match to patent applicant names using fuzzy matching\n",
    "3. Validate with DISCERN 2 where available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder: DISCERN 2 Integration\n",
    "\n",
    "**Note:** DISCERN 2 data files are not included in this repository. \n",
    "\n",
    "**To integrate DISCERN 2:**\n",
    "1. Download from https://zenodo.org/records/13619821\n",
    "2. Extract relevant tables (consult data dictionary)\n",
    "3. Load into DuckDB or pandas\n",
    "4. Implement join logic below\n",
    "\n",
    "**For now, we'll use the clinical trials dataset's existing gvkey mapping as a proxy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sponsor name → gvkey lookup from clinical trials\n",
    "sponsor_gvkey_map = (\n",
    "    clinical_trials[['sponsor_name', 'gvkey_sponsor']]\n",
    "    .drop_duplicates()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "print(f\"Sponsor-GVKEY mapping created: {len(sponsor_gvkey_map)} unique mappings\")\n",
    "print(f\"\\nSample mappings:\")\n",
    "sponsor_gvkey_map.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Clean organization names for matching\n",
    "def clean_org_name(name):\n",
    "    \"\"\"\n",
    "    Standardize organization names for matching.\n",
    "    \n",
    "    Removes:\n",
    "    - Legal suffixes (Inc., Corp., Ltd., etc.)\n",
    "    - Punctuation\n",
    "    - Extra whitespace\n",
    "    \n",
    "    Converts to lowercase.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return ''\n",
    "    \n",
    "    name = str(name).lower()\n",
    "    \n",
    "    # Remove legal suffixes\n",
    "    suffixes = [\n",
    "        r'\\binc\\.?\\b', r'\\bcorp\\.?\\b', r'\\bcorporation\\b',\n",
    "        r'\\bltd\\.?\\b', r'\\blimited\\b', r'\\bco\\.?\\b',\n",
    "        r'\\bllc\\b', r'\\blp\\b', r'\\bplc\\b',\n",
    "        r'\\bsa\\b', r'\\bag\\b', r'\\bgmbh\\b'\n",
    "    ]\n",
    "    for suffix in suffixes:\n",
    "        name = re.sub(suffix, '', name)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    name = re.sub(r'[^a-z0-9\\s]', ' ', name)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    name = ' '.join(name.split())\n",
    "    \n",
    "    return name.strip()\n",
    "\n",
    "# Test cleaning function\n",
    "test_names = [\n",
    "    'Pfizer Inc.',\n",
    "    'Bristol-Myers Squibb',\n",
    "    'Eli Lilly and Company',\n",
    "    'Novartis AG'\n",
    "]\n",
    "\n",
    "print(\"Name cleaning examples:\")\n",
    "for name in test_names:\n",
    "    print(f\"  {name:30s} → {clean_org_name(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned name lookup\n",
    "sponsor_gvkey_map['sponsor_name_clean'] = sponsor_gvkey_map['sponsor_name'].apply(clean_org_name)\n",
    "\n",
    "# Remove duplicates after cleaning (some names may collapse to same cleaned version)\n",
    "sponsor_lookup = (\n",
    "    sponsor_gvkey_map\n",
    "    .groupby('sponsor_name_clean')['gvkey_sponsor']\n",
    "    .first()  # Take first gvkey if multiple matches\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Cleaned sponsor lookup created: {len(sponsor_lookup)} unique clean names\")\n",
    "print(f\"\\nSample lookup:\")\n",
    "for i, (clean_name, gvkey) in enumerate(list(sponsor_lookup.items())[:10]):\n",
    "    print(f\"  {clean_name:30s} → {gvkey}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Patent Applicants to GVKEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract applicants for applications in our time window\n",
    "print(\"Extracting applicants for 2000-2025 applications...\")\n",
    "\n",
    "applicants_df = con.execute(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        a.application_id,\n",
    "        a.filing_date,\n",
    "        a.filing_year,\n",
    "        a.patent_id,\n",
    "        p.applicant_organization\n",
    "    FROM applications_2000_2025 a\n",
    "    INNER JOIN pg_applicant p ON a.application_id = p.application_id\n",
    "    WHERE p.applicant_organization IS NOT NULL\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Extracted {len(applicants_df):,} application-applicant pairs\")\n",
    "print(f\"\\nSample:\")\n",
    "applicants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean applicant names and match to gvkey\n",
    "print(\"Matching applicant names to GVKEY...\")\n",
    "\n",
    "applicants_df['applicant_clean'] = applicants_df['applicant_organization'].apply(clean_org_name)\n",
    "applicants_df['gvkey'] = applicants_df['applicant_clean'].map(sponsor_lookup)\n",
    "\n",
    "# Check match rate\n",
    "match_rate = (applicants_df['gvkey'].notna().sum() / len(applicants_df)) * 100\n",
    "print(f\"\\nMatch rate: {match_rate:.2f}%\")\n",
    "print(f\"Matched applications: {applicants_df['gvkey'].notna().sum():,}\")\n",
    "print(f\"Unmatched applications: {applicants_df['gvkey'].isna().sum():,}\")\n",
    "\n",
    "# Filter to matched applications only\n",
    "matched_applications = applicants_df[applicants_df['gvkey'].notna()].copy()\n",
    "print(f\"\\n✓ Working with {len(matched_applications):,} matched applications\")\n",
    "\n",
    "matched_applications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coverage: which firms from clinical trials have patent applications?\n",
    "print(\"Firms with patent applications:\")\n",
    "\n",
    "firms_with_patents = matched_applications['gvkey'].unique()\n",
    "firms_in_trials = clinical_trials['gvkey_sponsor'].dropna().unique()\n",
    "\n",
    "coverage = (len(set(firms_with_patents) & set(firms_in_trials)) / len(firms_in_trials)) * 100\n",
    "\n",
    "print(f\"  Clinical trial firms: {len(firms_in_trials)}\")\n",
    "print(f\"  Firms with patents: {len(firms_with_patents)}\")\n",
    "print(f\"  Overlap: {len(set(firms_with_patents) & set(firms_in_trials))}\")\n",
    "print(f\"  Coverage: {coverage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AI Classification Logic\n",
    "\n",
    "### Two-Pronged AI Identification Strategy\n",
    "\n",
    "#### Method 1: CPC Classification Codes\n",
    "**High Precision Approach**\n",
    "\n",
    "AI-related CPC codes (from WIPO/EPO standards):\n",
    "- **G06N** - Computing based on specific computational models\n",
    "  - G06N3 - Neural networks\n",
    "  - G06N5 - Knowledge-based models\n",
    "  - G06N7 - Probabilistic/fuzzy logic\n",
    "  - G06N10 - Quantum computing\n",
    "  - G06N20 - Machine learning\n",
    "\n",
    "**Advantages:**\n",
    "- Examiner-assigned (authoritative)\n",
    "- Standardized internationally\n",
    "- High precision\n",
    "\n",
    "**Limitations:**\n",
    "- Only available for granted patents (not pending applications)\n",
    "- May miss emerging AI applications not yet classified\n",
    "\n",
    "#### Method 2: Keyword-Based Filtering\n",
    "**High Recall Approach**\n",
    "\n",
    "Search title/abstract for AI-related terms:\n",
    "- Core ML: machine learning, deep learning, neural network, artificial intelligence\n",
    "- Techniques: reinforcement learning, supervised learning, unsupervised learning\n",
    "- Models: random forest, gradient boosting, support vector machine\n",
    "- Applications: computer vision, natural language processing, NLP\n",
    "\n",
    "**Advantages:**\n",
    "- Works for both granted and pending applications\n",
    "- Captures emerging terminology\n",
    "- Higher recall\n",
    "\n",
    "**Limitations:**\n",
    "- May include false positives\n",
    "- Requires careful keyword curation\n",
    "\n",
    "### Recommended: Hybrid Approach\n",
    "- **Primary:** CPC codes (for granted patents)\n",
    "- **Secondary:** Keywords (for all applications, especially pending)\n",
    "- **Combined:** Mark as AI if either method flags it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AI-related CPC code patterns\n",
    "AI_CPC_PATTERNS = [\n",
    "    'G06N3',   # Neural networks\n",
    "    'G06N5',   # Knowledge-based models\n",
    "    'G06N7',   # Probabilistic/fuzzy logic\n",
    "    'G06N10',  # Quantum computing\n",
    "    'G06N20',  # Machine learning\n",
    "]\n",
    "\n",
    "# Define AI-related keywords (lowercase)\n",
    "AI_KEYWORDS = [\n",
    "    # Core ML terms\n",
    "    'machine learning', 'deep learning', 'neural network', 'artificial intelligence',\n",
    "    'ai model', 'ml model', 'deep neural', 'convolutional neural',\n",
    "    \n",
    "    # Learning paradigms\n",
    "    'supervised learning', 'unsupervised learning', 'reinforcement learning',\n",
    "    'semi-supervised', 'transfer learning', 'meta-learning',\n",
    "    \n",
    "    # Specific models\n",
    "    'random forest', 'gradient boosting', 'support vector machine', 'svm',\n",
    "    'decision tree', 'bayesian network', 'recurrent neural', 'lstm',\n",
    "    'transformer', 'attention mechanism', 'generative adversarial',\n",
    "    \n",
    "    # Applications\n",
    "    'computer vision', 'natural language processing', 'nlp',\n",
    "    'image recognition', 'speech recognition', 'predictive model',\n",
    "    \n",
    "    # Techniques\n",
    "    'feature extraction', 'dimensionality reduction', 'classification algorithm',\n",
    "    'regression algorithm', 'clustering algorithm'\n",
    "]\n",
    "\n",
    "print(f\"AI CPC patterns defined: {len(AI_CPC_PATTERNS)}\")\n",
    "print(f\"AI keywords defined: {len(AI_KEYWORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Check if text contains AI keywords\n",
    "def contains_ai_keywords(text):\n",
    "    \"\"\"\n",
    "    Check if text contains any AI-related keywords.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (bool, list of matched keywords)\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return False, []\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    matched_keywords = []\n",
    "    \n",
    "    for keyword in AI_KEYWORDS:\n",
    "        if keyword in text_lower:\n",
    "            matched_keywords.append(keyword)\n",
    "    \n",
    "    return len(matched_keywords) > 0, matched_keywords\n",
    "\n",
    "# Test function\n",
    "test_texts = [\n",
    "    \"A machine learning approach to drug discovery\",\n",
    "    \"Novel pharmaceutical composition\",\n",
    "    \"Deep neural network for protein folding prediction\"\n",
    "]\n",
    "\n",
    "print(\"Keyword detection test:\")\n",
    "for text in test_texts:\n",
    "    is_ai, keywords = contains_ai_keywords(text)\n",
    "    print(f\"  {text[:50]:50s} → AI: {is_ai}, Keywords: {keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Applications as AI-Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get CPC codes for granted patents\n",
    "print(\"Extracting CPC codes for matched applications...\")\n",
    "\n",
    "# Get patent IDs from matched applications (only granted ones have CPC codes)\n",
    "granted_patent_ids = matched_applications['patent_id'].dropna().unique()\n",
    "\n",
    "print(f\"Granted patents in matched set: {len(granted_patent_ids):,}\")\n",
    "\n",
    "# Query CPC codes (using Task1 data)\n",
    "cpc_codes_df = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        patent_id,\n",
    "        cpc_group\n",
    "    FROM g_cpc_current\n",
    "    WHERE patent_id IN ({','.join(\"'\" + str(pid) + \"'\" for pid in granted_patent_ids[:1000])})\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Extracted CPC codes for {cpc_codes_df['patent_id'].nunique():,} patents\")\n",
    "cpc_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify AI patents based on CPC codes\n",
    "def is_ai_cpc(cpc_group):\n",
    "    \"\"\"Check if CPC code matches AI patterns.\"\"\"\n",
    "    if pd.isna(cpc_group):\n",
    "        return False\n",
    "    for pattern in AI_CPC_PATTERNS:\n",
    "        if str(cpc_group).startswith(pattern):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "cpc_codes_df['is_ai_cpc'] = cpc_codes_df['cpc_group'].apply(is_ai_cpc)\n",
    "\n",
    "# Get AI patents by CPC\n",
    "ai_patents_cpc = cpc_codes_df[cpc_codes_df['is_ai_cpc']]['patent_id'].unique()\n",
    "print(f\"AI patents identified by CPC: {len(ai_patents_cpc):,}\")\n",
    "\n",
    "# Get AI CPC codes found\n",
    "ai_cpc_codes = (\n",
    "    cpc_codes_df[cpc_codes_df['is_ai_cpc']]\n",
    "    .groupby('patent_id')['cpc_group']\n",
    "    .apply(lambda x: ','.join(x))\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get titles/abstracts for keyword search\n",
    "print(\"Loading patent titles and abstracts...\")\n",
    "\n",
    "# Get titles from g_application (if available) or g_patent\n",
    "# For now, using Task1 g_patent table as proxy\n",
    "titles_abstracts = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        p.patent_id,\n",
    "        p.patent_title as title,\n",
    "        a.patent_abstract as abstract\n",
    "    FROM g_patent p\n",
    "    LEFT JOIN g_patent_abstract a ON p.patent_id = a.patent_id\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Loaded titles/abstracts for {len(titles_abstracts):,} patents\")\n",
    "titles_abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply keyword detection\n",
    "print(\"Detecting AI keywords in titles and abstracts...\")\n",
    "\n",
    "# Combine title and abstract for search\n",
    "titles_abstracts['combined_text'] = (\n",
    "    titles_abstracts['title'].fillna('') + ' ' + \n",
    "    titles_abstracts['abstract'].fillna('')\n",
    ")\n",
    "\n",
    "# Apply keyword detection (this may take a few minutes for large datasets)\n",
    "keyword_results = titles_abstracts['combined_text'].apply(contains_ai_keywords)\n",
    "titles_abstracts['is_ai_keyword'] = keyword_results.apply(lambda x: x[0])\n",
    "titles_abstracts['ai_keywords_found'] = keyword_results.apply(lambda x: ','.join(x[1]))\n",
    "\n",
    "ai_patents_keyword = titles_abstracts[titles_abstracts['is_ai_keyword']]['patent_id'].unique()\n",
    "print(f\"\\n✓ AI patents identified by keywords: {len(ai_patents_keyword):,}\")\n",
    "\n",
    "# Show sample AI patents\n",
    "print(\"\\nSample AI patents identified by keywords:\")\n",
    "titles_abstracts[titles_abstracts['is_ai_keyword']][['patent_id', 'title', 'ai_keywords_found']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CPC and keyword classifications\n",
    "print(\"Combining CPC and keyword classifications...\")\n",
    "\n",
    "# Merge back to matched_applications\n",
    "matched_applications['is_ai_cpc'] = matched_applications['patent_id'].isin(ai_patents_cpc)\n",
    "matched_applications = matched_applications.merge(\n",
    "    titles_abstracts[['patent_id', 'is_ai_keyword', 'ai_keywords_found', 'title']],\n",
    "    on='patent_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create combined AI flag\n",
    "matched_applications['is_ai'] = (\n",
    "    matched_applications['is_ai_cpc'] | \n",
    "    matched_applications['is_ai_keyword'].fillna(False)\n",
    ")\n",
    "\n",
    "# Add AI method indicator\n",
    "def get_ai_method(row):\n",
    "    if row['is_ai_cpc'] and row['is_ai_keyword']:\n",
    "        return 'both'\n",
    "    elif row['is_ai_cpc']:\n",
    "        return 'cpc'\n",
    "    elif row['is_ai_keyword']:\n",
    "        return 'keyword'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "matched_applications['ai_method'] = matched_applications.apply(get_ai_method, axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== AI CLASSIFICATION SUMMARY ===\")\n",
    "print(f\"Total applications: {len(matched_applications):,}\")\n",
    "print(f\"AI by CPC only: {(matched_applications['ai_method'] == 'cpc').sum():,}\")\n",
    "print(f\"AI by keyword only: {(matched_applications['ai_method'] == 'keyword').sum():,}\")\n",
    "print(f\"AI by both methods: {(matched_applications['ai_method'] == 'both').sum():,}\")\n",
    "print(f\"Total AI applications: {matched_applications['is_ai'].sum():,}\")\n",
    "print(f\"AI share: {(matched_applications['is_ai'].sum() / len(matched_applications) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Firm-Year Aggregation\n",
    "\n",
    "### Create Research-Ready Firm-Year Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to firm-year level\n",
    "print(\"Aggregating to firm-year level...\")\n",
    "\n",
    "firm_year = (\n",
    "    matched_applications\n",
    "    .groupby(['gvkey', 'filing_year'])\n",
    "    .agg({\n",
    "        'application_id': 'count',              # Total applications\n",
    "        'is_ai': 'sum'                          # AI applications\n",
    "    })\n",
    "    .rename(columns={\n",
    "        'application_id': 'total_applications',\n",
    "        'is_ai': 'ai_applications'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate derived metrics\n",
    "firm_year['ai_share'] = firm_year['ai_applications'] / firm_year['total_applications']\n",
    "firm_year['ai_dummy'] = (firm_year['ai_applications'] > 0).astype(int)\n",
    "\n",
    "# Rename filing_year to year for clarity\n",
    "firm_year = firm_year.rename(columns={'filing_year': 'year'})\n",
    "\n",
    "print(f\"✓ Firm-year dataset created: {len(firm_year):,} observations\")\n",
    "print(f\"  Unique firms: {firm_year['gvkey'].nunique()}\")\n",
    "print(f\"  Year range: {firm_year['year'].min()} - {firm_year['year'].max()}\")\n",
    "\n",
    "firm_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== FIRM-YEAR DATASET SUMMARY ===\")\n",
    "print(f\"\\nSample size: {len(firm_year):,} firm-year observations\")\n",
    "print(f\"\\nApplications per firm-year:\")\n",
    "print(firm_year['total_applications'].describe())\n",
    "print(f\"\\nAI applications per firm-year:\")\n",
    "print(firm_year['ai_applications'].describe())\n",
    "print(f\"\\nAI share distribution:\")\n",
    "print(firm_year['ai_share'].describe())\n",
    "print(f\"\\nFirm-years with at least one AI patent: {firm_year['ai_dummy'].sum():,} ({firm_year['ai_dummy'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal trends: AI adoption over time\n",
    "yearly_trends = (\n",
    "    firm_year\n",
    "    .groupby('year')\n",
    "    .agg({\n",
    "        'total_applications': 'sum',\n",
    "        'ai_applications': 'sum',\n",
    "        'ai_dummy': 'sum',  # Number of firms with AI\n",
    "        'gvkey': 'nunique'  # Number of firms\n",
    "    })\n",
    "    .rename(columns={'gvkey': 'num_firms', 'ai_dummy': 'firms_with_ai'})\n",
    ")\n",
    "\n",
    "yearly_trends['ai_share'] = yearly_trends['ai_applications'] / yearly_trends['total_applications']\n",
    "yearly_trends['firm_adoption_rate'] = yearly_trends['firms_with_ai'] / yearly_trends['num_firms']\n",
    "\n",
    "print(\"\\n=== TEMPORAL TRENDS ===\")\n",
    "print(yearly_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge with Clinical Trials Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clinical trials firm-year dataset\n",
    "print(\"Creating clinical trials firm-year dataset...\")\n",
    "\n",
    "trials_firm_year = (\n",
    "    clinical_trials\n",
    "    .groupby(['gvkey_sponsor', 'start_year'])\n",
    "    .agg({\n",
    "        'nct_id': 'count',\n",
    "        'phase_number': 'mean'  # Average phase\n",
    "    })\n",
    "    .rename(columns={\n",
    "        'nct_id': 'num_trials',\n",
    "        'phase_number': 'avg_phase'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={'gvkey_sponsor': 'gvkey', 'start_year': 'year'})\n",
    ")\n",
    "\n",
    "print(f\"✓ Clinical trials firm-year: {len(trials_firm_year):,} observations\")\n",
    "trials_firm_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge patent and trial datasets\n",
    "print(\"Merging patent applications with clinical trials...\")\n",
    "\n",
    "merged_firm_year = firm_year.merge(\n",
    "    trials_firm_year,\n",
    "    on=['gvkey', 'year'],\n",
    "    how='outer',  # Keep all firm-years from both datasets\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Fill NAs with 0 for count variables\n",
    "count_vars = ['total_applications', 'ai_applications', 'ai_dummy', 'num_trials']\n",
    "for var in count_vars:\n",
    "    merged_firm_year[var] = merged_firm_year[var].fillna(0).astype(int)\n",
    "\n",
    "# Recalculate ai_share\n",
    "merged_firm_year['ai_share'] = np.where(\n",
    "    merged_firm_year['total_applications'] > 0,\n",
    "    merged_firm_year['ai_applications'] / merged_firm_year['total_applications'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Merged dataset: {len(merged_firm_year):,} firm-year observations\")\n",
    "print(f\"\\nMerge statistics:\")\n",
    "print(merged_firm_year['_merge'].value_counts())\n",
    "\n",
    "# Drop merge indicator\n",
    "merged_firm_year = merged_firm_year.drop('_merge', axis=1)\n",
    "\n",
    "merged_firm_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of merged dataset\n",
    "print(\"=== MERGED FIRM-YEAR DATASET SUMMARY ===\")\n",
    "print(f\"\\nTotal observations: {len(merged_firm_year):,}\")\n",
    "print(f\"Unique firms: {merged_firm_year['gvkey'].nunique()}\")\n",
    "print(f\"Year range: {merged_firm_year['year'].min()} - {merged_firm_year['year'].max()}\")\n",
    "print(f\"\\nFirm-years with patents: {(merged_firm_year['total_applications'] > 0).sum():,}\")\n",
    "print(f\"Firm-years with AI patents: {(merged_firm_year['ai_applications'] > 0).sum():,}\")\n",
    "print(f\"Firm-years with trials: {(merged_firm_year['num_trials'] > 0).sum():,}\")\n",
    "print(f\"Firm-years with both patents and trials: {((merged_firm_year['total_applications'] > 0) & (merged_firm_year['num_trials'] > 0)).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export patent-level dataset\n",
    "patent_level_output = matched_applications[[\n",
    "    'application_id', 'patent_id', 'filing_date', 'filing_year',\n",
    "    'gvkey', 'applicant_organization', 'applicant_clean',\n",
    "    'is_ai', 'ai_method', 'is_ai_cpc', 'is_ai_keyword', 'ai_keywords_found',\n",
    "    'title'\n",
    "]].copy()\n",
    "\n",
    "patent_level_output.to_csv('patent_level_dataset.csv', index=False)\n",
    "print(f\"✓ Exported patent-level dataset: patent_level_dataset.csv\")\n",
    "print(f\"  Shape: {patent_level_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export firm-year dataset (patents only)\n",
    "firm_year.to_csv('firm_year_patents.csv', index=False)\n",
    "print(f\"✓ Exported firm-year patent dataset: firm_year_patents.csv\")\n",
    "print(f\"  Shape: {firm_year.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export merged firm-year dataset (patents + trials)\n",
    "merged_firm_year.to_csv('firm_year_merged.csv', index=False)\n",
    "print(f\"✓ Exported merged firm-year dataset: firm_year_merged.csv\")\n",
    "print(f\"  Shape: {merged_firm_year.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Deliverables Created\n",
    "\n",
    "1. **patent_level_dataset.csv**\n",
    "   - One row per patent application\n",
    "   - Contains AI classification flags and methods\n",
    "   - ~{patent_level_output.shape[0]:,} applications\n",
    "\n",
    "2. **firm_year_patents.csv**\n",
    "   - One row per gvkey-year\n",
    "   - Patent application metrics: total, AI count, AI share\n",
    "   - ~{firm_year.shape[0]:,} firm-year observations\n",
    "\n",
    "3. **firm_year_merged.csv**\n",
    "   - Combined patent applications + clinical trials\n",
    "   - Ready for regression analysis\n",
    "   - ~{merged_firm_year.shape[0]:,} firm-year observations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- **Match Rate:** {match_rate:.1f}% of patent applications matched to clinical trial firms\n",
    "- **AI Patents:** {matched_applications['is_ai'].sum():,} AI-related applications identified\n",
    "- **AI Share:** {(matched_applications['is_ai'].mean()*100):.2f}% of applications are AI-related\n",
    "- **Temporal Coverage:** 2000-2025\n",
    "\n",
    "### Recommended Next Steps\n",
    "\n",
    "1. **DISCERN 2 Integration**\n",
    "   - Download DISCERN 2 database\n",
    "   - Improve gvkey matching coverage\n",
    "   - Handle time-varying firm identifiers (M&A)\n",
    "\n",
    "2. **Validation**\n",
    "   - Manual review of AI classification accuracy\n",
    "   - Compare to known AI patents/firms\n",
    "   - Refine keyword list based on false positives\n",
    "\n",
    "3. **Extended Analysis**\n",
    "   - Lag structures (patents → trials)\n",
    "   - Firm-specific AI intensity trends\n",
    "   - Technology subfield analysis (drug discovery vs. clinical trial AI)\n",
    "\n",
    "4. **PubMed Linkage** (Task #2 Part 2)\n",
    "   - Implement NCT ID → PubMed search\n",
    "   - Identify AI methods in trial publications\n",
    "\n",
    "### Memory Efficiency Notes\n",
    "\n",
    "**For larger datasets:**\n",
    "1. Process year-by-year in chunks\n",
    "2. Use DuckDB for all filtering/aggregation\n",
    "3. Keep only necessary columns in memory\n",
    "4. Use categorical dtypes for string columns\n",
    "5. Consider Dask/Vaex for very large datasets (>50GB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
